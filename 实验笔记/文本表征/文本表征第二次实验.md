注：本次实验使用 enwiki8 (http://mattmahoney.net/dc/enwik8.zip) 语料库，及https://github.com/deborausujono/word2vecpy的模型
## 性能结果
相似度计算使用余弦相似度

Spearman-correlation如下：

| 方法  | CBOW | SG   |
| --- | ---- | ---- |
| HS  | 0.24 | 0.57 |
| NS  | 0.23 | 0.36 |
其他超参数
• 向量维度:200
• 窗口大小:+/- 5词
• 词频过滤:5
- Number of negative examples:5
## 分析比较
在控制其他变量不变的情况下，SG得到的向量的相似度，比CBOW的向量更与标准数据相关，即**向量数据SG更好**，但相应的，**计算时间SG也比CBOW长**。
这可能是因为CBOW是通过上下文来预测一个词，这样学习的次数比较少，在遇到生僻词的时候效果比较差，而SG是通过一个词预测上下文，学习的次数比较多，可以更好的处理生僻词

在控制其他变量不变的情况下，HS比NS更好，但在CBOW时无明显优势。
HS使用了一个二叉树（通常是霍夫曼树）来计算词的概率，而NS通过随机抽取负样本来近似整个词汇表的概率分布。HS通常在处理大词汇表时更有效，而NS在小词汇表或者高质量负样本的情况下表现更好，这就能说明HS在SG时优势明显，而在处理CBOW时无明显优势。

## 实验过程
==模型实现==

1. **初始化**：模型首先初始化词向量。在这段代码中，`syn0` 和 `syn1` 是词向量的初始化值。`syn0` 是输入词向量，`syn1` 是输出词向量。这两个向量都是随机初始化的。
![](https://raw.githubusercontent.com/ustc21xyx/picture-bed/main/20240423172802.png)

2. **构建词汇表**：模型读取训练数据，构建词汇表，并统计每个词的出现次数。
   使用`vacab`类实现词汇表
   ![](https://raw.githubusercontent.com/ustc21xyx/picture-bed/main/20240423172702.png)
3. **训练**：模型会遍历训练数据中的每一个词，对每一个词，模型会查找它的上下文词（在 CBOW 中）或预测它的上下文词（在 Skip-gram 中）。模型会计算损失函数关于词向量的梯度，然后使用这个梯度来更新词向量。这个过程使用梯度下降法来完成。在`train_process`中完成，对于不同Method，使用不同过程，比如参数NS=0时，使用Huffman树

```ad-note
该模型使用了并行计算来缩短计算时间，在train()中有如下代码
```
![](https://raw.githubusercontent.com/ustc21xyx/picture-bed/main/20240423172617.png)

==数据处理==
模型接受参数，输出一个含有Vec表的文本文档model.txt
使用`similarity.py`处理两个word在输出模型中的相似度，采用余弦相似度，并且把余弦值从[-1,1]映射到[0,1]，以直观的相似度比较
![](https://raw.githubusercontent.com/ustc21xyx/picture-bed/main/20240423172448.png)


使用`spearman_correlation.py`来获得wordsim_similarity_goldstandard.txt的词汇，并不断调用similarity.py来每对词汇的相似度，最终计算与标准相似度的Spearman-correlation。

![](https://raw.githubusercontent.com/ustc21xyx/picture-bed/main/20240423172514.png)