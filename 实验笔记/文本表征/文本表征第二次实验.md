注：本次实验使用 enwiki8 (http://mattmahoney.net/dc/enwik8.zip) 语料库，及https://github.com/deborausujono/word2vecpy的模型
## 性能结果
相似度计算使用余弦相似度

Spearman-correlation如下：

| 方法  | CBOW | SG   |
| --- | ---- | ---- |
| HS  | 0.24 | 0.57 |
| NS  | 0.23 | 0.36 |
其他超参数
• 向量维度:200
• 窗口大小:+/- 5词
• 词频过滤:5
- Number of negative examples:5
## 分析比较
在控制其他变量不变的情况下，SG得到的向量的相似度，比CBOW的向量更与标准数据相关，即**向量数据SG更好**，但相应的，**计算时间SG也比CBOW长**。
这可能是因为CBOW是通过上下文来预测一个词，这样学习的次数比较少，在遇到生僻词的时候效果比较差，而SG是通过一个词预测上下文，学习的次数比较多，可以更好的处理生僻词

在控制其他变量不变的情况下，HS比NS更好，但在CBOW时无明显优势。
HS使用了一个二叉树（通常是霍夫曼树）来计算词的概率，而NS通过随机抽取负样本来近似整个词汇表的概率分布。HS通常在处理大词汇表时更有效，而NS在小词汇表或者高质量负样本的情况下表现更好，而在处理CBOW时无明显优势。

## 实验过程
==模型实现==

1. **初始化**：模型首先初始化词向量。在这段代码中，`syn0` 和 `syn1` 是词向量的初始化值。`syn0` 是输入词向量，`syn1` 是输出词向量。这两个向量都是随机初始化的。
```
def __init_process(*args):

global vocab, syn0, syn1, table, cbow, neg, dim, starting_alpha

global win, num_processes, global_word_count, fi

vocab, syn0_tmp, syn1_tmp, table, cbow, neg, dim, starting_alpha, win, num_processes, global_word_count = args[:-1]

fi = open(args[-1], 'r')

with warnings.catch_warnings():

warnings.simplefilter('ignore', RuntimeWarning)

syn0 = np.ctypeslib.as_array(syn0_tmp)

syn1 = np.ctypeslib.as_array(syn1_tmp)
```

2. **构建词汇表**：模型读取训练数据，构建词汇表，并统计每个词的出现次数。
   使用`vacab`类实现词汇表
   
3. **训练**：模型会遍历训练数据中的每一个词，对每一个词，模型会查找它的上下文词（在 CBOW 中）或预测它的上下文词（在 Skip-gram 中）。模型会计算损失函数关于词向量的梯度，然后使用这个梯度来更新词向量。这个过程使用梯度下降法来完成。在`train_process`中完成，对于不同Method，使用不同过程，比如参数NS=0时，使用Huffman树

```ad-note
该模型使用了并行计算来缩短计算时间，在train()中有如下代码


t0 = time.time()

pool = Pool(processes=num_processes, initializer=__init_process,

initargs=(vocab, syn0, syn1, table, cbow, neg, dim, alpha,

win, num_processes, global_word_count, fi))

pool.map(train_process, range(num_processes))

t1 = time.time()

```

==数据处理==
模型接受参数，输出一个含有Vec表的文本文档model.txt
使用simirity.py处理两个word在输出模型中的相似度
